---
layout: post
title:  Scraping via APIs
date: `r Sys.time()`
published: true
tags: [r, httr, web-scraping]
categories: [programming]
---

<STYLE TYPE="text/css"> 
<!-- 
.hangingindent {
  padding-left: 80px ;
  padding-right: 80px ;
  text-indent: -32px ;
}
--> 
</STYLE>

<a href="http://bradleyboehmke.github.io/2016/01/scraping-via-apis.html"><img src="https://d15n4q3o4x3svq.cloudfront.net/assets/tutorials/curl/api-a397cc184c5622fb5130af1b7baf149d.png" alt="Scraping with APIs" style="float:left; margin:0px 8px 0px 0px; width: 17%; height: 17%;"></a>
In the epic poem *Rime of the Ancient Mariner*, Samuel Taylor Coleridge states, “Water, water, everywhere, nor any a drop to drink.” Indeed, some would say the same about data. Data appear to be [everywhere](http://www.technologyreview.com/view/530371/big-data-creating-the-power-to-move-heaven-and-earth/) yet only a [fraction are analyzed](https://gigaom.com/2013/03/10/the-big-data-world-is-operating-at-1-percent/). There are several [arguments](http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation) as to why but one that has reached the concern of the [White House](https://www.whitehouse.gov/sites/default/files/omb/assets/memoranda_2010/m10-06.pdf) is data accessibility.  However, this is rapidly changing as growth in technology and resources are quickly opening the doors of many data vaults to the masses. We, the public minions, now have access to a wide range of data; from social, financial, government, and ecommerce data to geospatial, search engine, and even ant data. We just need to know how to *get* it. Enter APIs.<!--more-->  



<br>

## tl;dr
Here's what you should get out of this post: 

<p class="hangingindent"><SPAN STYLE="font-size: 20pt"><a href="#what_api" style="color:black">&#9312;</a></SPAN>  What's an API? Just another way to communicate from R to a web-based database for a consistent data retrieval process.</p>

<p class="hangingindent"><SPAN STYLE="font-size: 20pt"><a href="#needs_api" style="color:black">&#9313;</a></SPAN>   Prereqs? You'll need some metadata on the data you want (gotta know a little something about what you're trying to pull). And depending on the API you'll typically need a key (aka token) or OAuth.</p>

<p class="hangingindent"><SPAN STYLE="font-size: 20pt"><a href="#existing_api" style="color:black">&#9314;</a></SPAN> There's probably an <strike>app</strike> package for that. There's a slew of existing R API packages already built to access databases. I quickly cover 3 packages to give you a taste for how they typically work.</p>

<p class="hangingindent"><SPAN STYLE="font-size: 20pt"><a href="#httr_api" style="color:black">&#9315;</a></SPAN> What if the API that you want data from does not yet have an R package developed for it? Use the `httr` package. I demonstrate how to use `httr` to request API data...both with and without OAuth. </p>


<br>


<a name="what_api"></a>

## &#9312; What's an API?
An [application-programming interface (API)](https://en.wikipedia.org/wiki/Application_programming_interface) in a nutshell is a method of communication between software programs.  APIs allow programs to interact and use each other's functions by acting as a middle man. Why is this useful? Lets say you want to pull weather data from the [NOAA](http://www.ncdc.noaa.gov/cdo-web/webservices).  You have a few options: 

- You could query the data and download the spreadsheet or manually cut-n-paste the desired data and then import into R. Doesn't get you any coolness points. 
- You could use some webscraping techniques previously covered [here](http://bradleyboehmke.github.io/2015/12/scraping-tabular-and-excel-files-stored-online.html), [here](http://bradleyboehmke.github.io/2015/12/scraping-html-text.html), and [here](http://bradleyboehmke.github.io/2015/12/scraping-html-tables.html) to parse the desired data. Golf clap. The downfall of this strategy is if NOAA changes their website structure down the road your code will need to be adjusted.
- Or, you can use the [`rnoaa`](https://ropensci.org/tutorials/rnoaa_tutorial.html) package which allows you to send specific instructions to the NOAA API via R, the API will then perform the action requested and return the desired information. The benefit of this strategy is if the NOAA changes its website structure it won't impact the API data retreival structure which means no impact to your code. Standing ovation!

Consequently, APIs provide consistency in data retrieval processes which can be essential for recurring analyses. Luckily, the use of APIs by organizations that collect data are [growing exponentially](http://www.programmableweb.com/api-research). This is great for you and I as more and more data continues to be at our finger tips.  

So what do you need to get started?


<small><a href="#">Go to top</a></small>

<br>

<a name="needs_api"></a>

## &#9313; Prereqs?
Each API is unique; however, there are a few fundamental pieces of information you'll need to work with an API.  First, the reason you're using an API is to request specific types of data from a specific data set from a specific organization. You at least need to know a little something about each one of these:

1. The URL for the organization and data you are pulling. Most pre-built API packages already have this connection established but when using `httr` you'll need to specify.
2. The data set you are trying to pull from. Most organizations have numerous data sets to peruse so you need to make yourself familiar with the names of the available data sets.
3. The data content. You'll need to specify the specific data variables you want the API to retrieve so you'll need to be familiar with, or have access to, the data library.

In addition to these key components you will also, typically, need to provide a form of identification and/or authorization.  This is done via:

4. API key (aka token). A key is used to identify the user along with track and control how the API is being used (guard against malicious use). A key is often obtained by supplying basic information (i.e. name, email) to the organization and in return they give you a multi-digit key.
5. [OAuth](http://oauth.net/). OAuth is an authorization framework that provides credentials as proof for access to certain information. Multiple forms of credentials exist and OAuth can actually be a fairly confusing topic; however, the `httr` package has simplified this greatly which we demonstrate <a href="#httr_api">later</a> in this post.

Rather than dwell on these components, they'll likely become clearer as we progress through examples. So, let's move on to the fun stuff. 


<small><a href="#">Go to top</a></small>

<br>

<a name="existing_api"></a>

## &#9314; Existing API Packages
Like everything else you do in R, when looking to work with an API your first question should be "Is there a package for that?" R has an extensive list of packages in which API data feeds have been hooked into R. You can find a slew of them scattered throughout the [CRAN Task View: Web Technologies and Services](https://cran.r-project.org/web/views/WebTechnologies.html) web page, on the [rOpenSci](https://ropensci.org/packages/) web page, and some more [here](http://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r). 

To give you a taste for how these packages typically work, I'll quickly cover three packages:

- <a href="#blsAPI">`blsAPI`</a> for pulling U.S. Bureau of Labor Statistics data
- <a href="#rnoaa">`rnoaa`</a> for pulling NOAA climate data
- <a href="#rtimes">`rtimes`</a> for pulling data from multiple APIs offered by the New York Times 

<a name="blsAPI"></a>

### blsAPI
The [`blsAPI`](https://cran.r-project.org/web/packages/blsAPI/index.html) allows users to request data for one or multiple series through the U.S. Bureau of Labor Statistics API. To use the `blsAPI` app you only need knowledge on the data; no key or OAuth are required. I lllustrate by pulling [Mass Layoff Statistics](http://www.bls.gov/mls/mlsover.htm) data but you will find all the available data sets and their series code information [here](http://www.bls.gov/help/hlpforma.htm). 

The key information you will be concerned about is contained in the series identifier.  For the Mass Layoff data the the series ID code  is MLUMS00NN0001003. Each component of this series code has meaning and can be adjusted to get specific Mass Layoff data.  The BLS provides this [breakdown](http://www.bls.gov/help/hlpforma.htm#ML) for what each component means along with the available list of codes for this data set.  For instance, the **S00** (MLUM**S00**NN0001003) component represents the [division/state](http://download.bls.gov/pub/time.series/ml/ml.srd). S00 will pull for all states but I could change to D30 to pull data for the Midwest or S39 to pull for Ohio. The **N0001** (MLUMS00N**N0001**003) component represents the [industry/demographics](http://download.bls.gov/pub/time.series/ml/ml.irc). N0001 pulls data for all industries but I could change to N0008 to pull data for the food industry or C00A2 for all persons age 30-44. 

I simply call the series identifier in the `blsAPI()` function which pulls the JSON data object.  We can then use the `fromJSON()` function from the `rjson` package to convert to an R data object (a list in this case). You can see that the raw data pull provides a list of 4 items.  The first three provide some metadata info (status, response time, and message if applicable). The data we are concerned about is in the 4th (Results&#36;series&#36;data) list item which contains 31 observations.

```{r, warning=FALSE, message=FALSE, collapse=TRUE, cache=TRUE}
library(rjson)
library(blsAPI)

# supply series identifier to pull data (initial pull is in JSON data)
layoffs_json <- blsAPI('MLUMS00NN0001003') 

# convert from JSON into R object
layoffs <- fromJSON(layoffs_json)                   
```
```{r, eval=FALSE}
List of 4
 $ status      : chr "REQUEST_SUCCEEDED"
 $ responseTime: num 38
 $ message     : list()
 $ Results     :List of 1
  ..$ series:List of 1
  .. ..$ :List of 2
  .. .. ..$ seriesID: chr "MLUMS00NN0001003"
  .. .. ..$ data    :List of 31
  .. .. .. ..$ :List of 5
  .. .. .. .. ..$ year      : chr "2013"
  .. .. .. .. ..$ period    : chr "M05"
  .. .. .. .. ..$ periodName: chr "May"
  .. .. .. .. ..$ value     : chr "1383"
```

One of the inconveniences of an API is we do not get to specify how the data we receive is formatted. This is a minor price to pay considering all the other benefits APIs provide. Once we understand the received data format we can typically re-format using a little subsetting and looping.

```{r, warning=FALSE, message=FALSE, collapse=TRUE}

# create empty data frame to fill  
layoff_df <- data.frame(NULL)

# extract data of interest from each nested year-month list  
for(i in seq_along(layoffs$Results$series[[1]]$data)) {
        df <- data.frame(layoffs$Results$series[[1]]$data[i][[1]][1:4])
        layoff_df <- rbind(layoff_df, df)
}

head(layoff_df)
```

`blsAPI` also allows you to pull multiple data series and has optional arguments (i.e. start year, end year, etc.). You can see other options at `help(package = blsAPI)`.

<a name="rnoaa"></a>

### rnoaa
The [`rnoaa`](https://ropensci.org/tutorials/rnoaa_tutorial.html) package allows users to request climate data from multiple data sets through the [National Climatic Data Center API](http://www.ncdc.noaa.gov/cdo-web/webservices/v2). Unlike `blsAPI`, the `rnoaa` app requires you to have an API key.  To request a key go [here](http://www.ncdc.noaa.gov/cdo-web/token) and provide your email; a key will immediately be emailed to you. 

```{r, echo=FALSE, cache=TRUE}
key <- "vXTdwNoAVxHemeARwOQeFbtmxTbdQdtM"
```

```{r, eval=FALSE}
key <- "vXTdwNoAVx..." # truncated 
```

With the key in hand, we can begin pulling data.  The NOAA provides a comprehensive [metadata library](http://www.ncdc.noaa.gov/homr/reports) to familiarize yourself with the data available. Let's start by pulling all the available NOAA climate stations near my residence. I live in Montgomery county Ohio so we can find all the stations in this county by inserting the [FIPS code](http://www.census.gov/geo/reference/codes/cou.html). Furthermore, I'm interested in stations that provide data for the [`GHCND` data set](https://www.ncdc.noaa.gov/oa/climate/ghcn-daily/) which contains records on numerous daily variables such as "maximum and minimum temperature, total daily precipitation, snowfall, and snow depth; however, about two thirds of the stations report precipitation only." See `?ncdc_stations` for other data sets available via `rnoaa`.

```{r, echo=TRUE, cache=TRUE, collapse=TRUE}
library(rnoaa)

stations <- ncdc_stations(datasetid='GHCND', 
              locationid='FIPS:39113',
              token = key)

stations$data
```

So we see that several stations are available from which to pull data. To actually pull data from one of these stations we need the station ID.  The station I want to pull data from is the Dayton International Airport station.  We can see that this station provides data from 1948-present and I can get the station ID as illustrated.

```{r, echo=TRUE, cache=TRUE, collapse=TRUE, message=FALSE}
library(dplyr)

stations$data %>% 
        filter(name == "DAYTON INTERNATIONAL AIRPORT, OH US") %>% 
        select(mindate, maxdate, id)
```

To pull all available GHCND data from this station we'll use `ncdc()`.  We simply supply the data to pull, the start and end dates (`ncdc` restricts you to a one year limit), station ID, and your key. We can see that this station provides a full range of data types.

```{r, echo=TRUE, cache=TRUE, collapse=TRUE}
climate <- ncdc(datasetid='GHCND', 
            startdate = '2015-01-01', 
            enddate = '2016-01-01', 
            stationid='GHCND:USW00093815',
            token = key)

climate$data
```

Since we recently had some snow here let's pull data on snow fall for 2015. We adjust the limit argument (by default `ncdc` limits results to 25) and identify the data type we want.  By sorting we see what days experienced the greatest snowfall (don't worry, the results are reported in mm!).

```{r, echo=TRUE, cache=TRUE, collapse=TRUE}
snow <- ncdc(datasetid='GHCND', 
            startdate = '2015-01-01', 
            enddate = '2015-12-31', 
            limit = 365,
            stationid='GHCND:USW00093815',
            datatypeid = 'SNOW',
            token = key)

snow$data %>% 
        arrange(desc(value))
```

This is just an intro to `rnoaa` as the package offers a slew of data sets to pull from and functions to apply.  It even offers built in plotting functions. Use `help(package = "rnoaa")` to see all that `rnoaa` has to offer.

<a name="rtimes"></a>

### rtimes
The [`rtimes`](https://cran.r-project.org/web/packages/rtimes/index.html) package provides an interface to Congress, Campaign Finance, Article Search, and Geographic APIs offered by the New York Times. The data libraries and documentation for the several APIs available can be found [here](http://developer.nytimes.com/docs/). To use the Times' API you'll need to get an API key [here](http://developer.nytimes.com/apps/register).

```{r, echo=FALSE, cache=TRUE}
article_key <- "4f23572d831dc4b1dfdeed03c14064fc:5:74007501"
cfinance_key <- "ee0b7cef3fde55df8aa0aca7f046717d:14:74007501"
congress_key <- "57b3e8a369a01db3a0646c15d99a597b:2:74007501"
```

```{r, eval=FALSE}
article_key <- "4f23572d8..."     # truncated
cfinance_key <- "ee0b7cef..."     # truncated
congress_key <- "57b3e8a3..."     # truncated
```

Lets start by searching NY Times articles. With the presendential elections upon us, we can illustrate by searching the least controversial candidate...Donald Trump. We can see that there are 4,566 article hits for the term "Trump". We can get more information on a particular article by subsetting.

```{r, message=FALSE, cache=TRUE, collapse=TRUE}
library(rtimes)

# article search for the term 'Trump'
articles <- as_search(q = "Trump", 
                 begin_date = "20150101", 
                 end_date = '20160101',
                 key = article_key)

# summary
articles$meta

# pull info on 3rd article
articles$data[3]
```

We can use the campaign finance API and functions to gain some insight into Trumps compaign income and expenditures. The only special data you need is the [FEC ID](http://www.fec.gov/finance/disclosure/candcmte_info.shtml?tabIndex=2) for the candidate of interest.

```{r, message=FALSE, cache=TRUE, collapse=TRUE}
trump <- cf_candidate_details(campaign_cycle = 2016, 
                     fec_id = 'P80001571',
                     key = cfinance_key)

# pull summary data
trump$meta
```

`rtimes` also allows us to gain some insight into what our locally elected officials are up to with the Congress API. First, I can get some informaton on my Senator and then use that information to see if he's supporting my interest. For instance, I can pull the most recent bills that he is co-sponsoring.

```{r, message=FALSE, cache=TRUE, collapse=TRUE}
# pull info on OH senator
senator <- cg_memberbystatedistrict(chamber = "senate", 
                                    state = "OH", 
                                    key = congress_key)
senator$meta

# use member ID to pull recent bill sponsorship
bills <- cg_billscosponsor(memberid = "B000944", 
                           type = "cosponsored", 
                           key = congress_key)
head(bills$data)
```

It looks like the most recent bill Sherrod is co-sponsoring is S.2098 - Student Right to Know Before You Go Act.  Maybe I'll do a NY Times article search with `as_search()` to find out more about this bill...an exercise for another time.

So this gives you some flavor of how these API packages work.  You typically need to know the data sets and variables requested along with an API key. But once you get these basics its pretty straight forward on requesting the data.  Your next question may be, what if the API that I want to get data from does not yet have an R package developed for it?


<small><a href="#">Go to top</a></small>

<br>



<a name="httr_api"></a>

## &#9315; httr for All Things Else
Although numerous R API packages are available, and cover a wide range of data, you may eventually run into a situation where you want to leverage an organization's API but an R package does not exist. Enter [`httr`](https://cran.r-project.org/web/packages/httr/index.html).  `httr` was developed by Hadley Wickham to easily work with web APIs. It offers multiple functions (i.e. `HEAD()`, `POST()`, `PATCH()`, `PUT()` and `DELETE()`); however, the function we are most concerned with today is `Get()`. We use the `Get()` function to access an API, provide it some request parameters, and receive an output. 

To give you a taste for how the `httr` package works, I'll quickly cover how to use it for a basic key-only API and an OAuth-required API:

- <a href="#key_only">`Key-only API`</a> is illustrated by pulling U.S. Department of Education data available on [data.gov](https://api.data.gov/docs/)
- <a href="#oauth">`OAuth-required API`</a> is  illustrated by pulling...

<a name="key_only"></a>

### Key-only API
To demonstrate how to use the `httr` package for accessing a key-only API, I'll illustrate with the [College Scorecard API](https://api.data.gov/docs/ed/) provided by the Department of Education. First, you'll need to [request your API key](https://api.data.gov/signup/). 

```{r, echo=FALSE, cache=TRUE}
edu_key <- "fd783wmS3Z3O9sJcAlS0WUpZT8Xy3KECigbgFY5I"
```

```{r, eval=FALSE}
edu_key <- "fd783wmS3Z..."     # truncated
```

We can now proceed to use `httr` to request data from the API with the `GET()` function.  I went to North Dakota State University (NDSU) for my undergrad so I'm interested in pulling some data for this school. I can use the provided [data library](https://collegescorecard.ed.gov/data/documentation/) and [query explanation](https://github.com/18F/open-data-maker/blob/api-docs/API.md) to determine the parameters required.  In this example, the `URL` includes the primary path ("https://api.data.gov/ed/collegescorecard/"), the API version ("v1"), and the endpoint ("schools"). The question mark ("?") at the end of the URL is included to begin the list of query parameters, which only includes my API key and the school of interest.

```{r, cache=TRUE}
library(httr)

URL <- "https://api.data.gov/ed/collegescorecard/v1/schools?"

# import all available data for NDSU
ndsu_req <- GET(URL, query = list(api_key = edu_key,
                                  school.name = "North Dakota State University"))
```

This request provides me with every piece of information collected by the U.S. Department of Education for NDSU. To retrieve the contents of this request I use the `content()` function which will output the data as an R object (a list in this case).  The data is segmented into two main components: *metadata* and *results*. I'm primarily interested in the results.

The results branch of this list provides information on lat-long location, school identifier codes, some basic info on the school (city, number of branches, school website, accreditor, etc.), and then student data for the years 1997-2013. 

```{r, cache=TRUE, collapse=TRUE}
ndsu_data <- content(ndsu_req)

names(ndsu_data)

names(ndsu_data$results[[1]])
```

To see what kind of student data categories are offered we can assess a single year. You can see that available data includes earnings, academics, student info/demographics, admissions, costs, etc. With such a large data set, which includes many embedded lists, sometimes the easiest way to learn the data structure is to peruse names at different levels. 

```{r, cache=TRUE, collapse=TRUE}
# student data categories available by year
names(ndsu_data$results[[1]]$`2013`)

# cost categories available by year
names(ndsu_data$results[[1]]$`2013`$cost)

# Avg net price cost categories available by year
names(ndsu_data$results[[1]]$`2013`$cost$avg_net_price)
```

So if I'm interested in comparing the rise in cost versus the rise in student debt I can simply pull this data once I've identified its location and naming structure.  

```{r, cache=TRUE, collapse=TRUE}
library(magrittr)

# subset list for annual student data only
ndsu_yr <- ndsu_data$results[[1]][c(as.character(1996:2013))]

# extract median debt data for each year
ndsu_yr %>%
        sapply(function(x) x$aid$median_debt$completers$overall) %>% 
        unlist()

# extract net price for each year
ndsu_yr %>% 
        sapply(function(x) x$cost$avg_net_price$overall) %>% 
        unlist()
```

Quite simple isn't it...at least once you've learned how the query requests are formatted for a particular API. 


<a name="oauth"></a>

### OAuth-required API
At the outset I mentioned how OAuth is an authorization framework that provides credentials as proof for access. Many APIs are open to the public and only require an API key; however, some APIs require authorization to account data (think personal Facebook & Twitter accounts). To access these accounts we must provide proper credentials and OAuth authentication allows us to do this. This post is not meant to explain the details of OAuth (for that see [this](http://hueniverse.com/2007/09/05/explaining-oauth/), [this](https://en.wikipedia.org/wiki/OAuth), and [this](http://hueniverse.com/oauth/)) but, rather, how to use `httr` in times when OAuth is required.

I'll demonstrate by accessing the Twitter API using my Twitter account. The first thing we need to do is identify the OAuth endpoints used to request access and authorization. To do this we can use `oauth_endpoint()` which typically requires a *request* URL, *authorization* URL, and *access* URL. `httr` also included some baked-in endpoints to include LinkedIn, Twitter, Vimeo, Google, Facebook, and GitHub. We can see the Twitter endpoints using the following:

```{r, cache=TRUE, collapse=TRUE}
twitter_endpts <- oauth_endpoints("twitter")
twitter_endpts
```

Next, I register my application at [https://apps.twitter.com/](https://apps.twitter.com/).  One thing to note is during the registration process, it will ask you for the *callback url*; be sure to use "http://127.0.0.1:1410". Once registered, Twitter will provide you with keys and access tokens. The two we are concerned about are the API key and API Secret.

```{r, echo=FALSE, cache=TRUE}
twitter_key <- "BZgukbColErm3fLvIgj9tXbIb"
twitter_secret <- "YpB8XymlgjxA2MGnzSpzMElrU0VqacpgoWS1K8EwdMVQHX6b4o"
```

```{r, eval=FALSE}
twitter_key <- "BZgukbCol..."   # truncated
twitter_secret <- "YpB8Xy..."   # truncated
```

We can then bundle the consumer key and secret into one object with `oauth_app()`. The first argument, `appname` is simply used as a local identifier; it does not need to match the name you gave the Twitter app you developed at https://apps.twitter.com/.

```{r, cache=TRUE, eval=TRUE, echo=FALSE}
twitter_app <- oauth_app(appname = "twitter",
                         key = twitter_key,
                         secret = twitter_secret
                         )
```

We are now ready to ask for access credentials. Since Twitter uses OAuth 1.0 we use `oauth1.0_token()` function and incorporate the endpoints identified and the `oauth_app` object we previously named `twitter_app`.

```{r, cache=TRUE, eval=FALSE, echo=TRUE, collapse=TRUE}
twitter_token <- oauth1.0_token(endpoint = twitter_endpts, twitter_app)

Waiting for authentication in browser...
Press Esc/Ctrl + C to abort
Authentication complete.
```

Once authentication is complete we can now use the API. I can pull all the tweets that show up on my personal timeline using the `GET()` function and the access cridentials I stored in `twitter_token`.  I then use `content()` to convert to a list and I can start to analyze the data.

In this case each tweet is saved as an individual list item and a full range of data are provided for each tweet (i.e. id, text, user, geo location, favorite count, etc). For instance, we can see that the first tweet was by [FiveThirtyEight](http://fivethirtyeight.com/) concerning American politics and, at the time of this analysis, has been favorited by 3 people.

```{r, cache=TRUE, eval=FALSE, echo=TRUE, collapse=TRUE}
# request Twitter data
req <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json",
           config(token = twitter_token))

# convert to R object
tweets <- content(req)

# available data for first tweet on my timeline
names(tweets[[1]])
 [1] "created_at"                    "id"                           
 [3] "id_str"                        "text"                         
 [5] "source"                        "truncated"                    
 [7] "in_reply_to_status_id"         "in_reply_to_status_id_str"    
 [9] "in_reply_to_user_id"           "in_reply_to_user_id_str"      
[11] "in_reply_to_screen_name"       "user"                         
[13] "geo"                           "coordinates"                  
[15] "place"                         "contributors"                 
[17] "is_quote_status"               "retweet_count"                
[19] "favorite_count"                "entities"                     
[21] "extended_entities"             "favorited"                    
[23] "retweeted"                     "possibly_sensitive"           
[25] "possibly_sensitive_appealable" "lang" 

# further analysis of first tweet on my timeline
tweets[[1]]$user$name
[1] "FiveThirtyEight"

tweets[[1]]$text
[1] "\U0001f3a7 A History Of Data In American Politics (Part 1): William Jennings Bryan to Barack Obama https://t.co/oCKzrXuRHf  https://t.co/6CvKKToxoH"

tweets[[1]]$favorite_count
[1] 3
```

This provides a fairly simple example of incorporating OAuth authorization. The `httr` provides several examples of accessing common social network APIs that require OAuth. I recommend you go through several of these examples to get familiar with using OAuth authorization; see them at `demo(package = "httr")`. The most difficult aspect of creating your own connections with APIs is gaining an understanding of the API and the arguments they leverage.  This obviously requires time and energy devoted to digging into the API documentation and data library. Next its just a matter of trial and error (likely more the latter than the former) to learn how to translate these arguments into `httr` function calls to pull the data of interest.

Also, note that `httr` provides several other useful functions not covered here for communicating with APIs (i.e. `POST()`, `BROWSE()`). For more on these other `httr` capabilities see this [quickstart vignette](https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html).


<small><a href="#">Go to top</a></small>

<br>


## Wrapping Up
As the growth in publicly available data continues, APIs appear to be the preferred medium for access. This will require analysts to become more familiar with interacting with APIs and the prerequisites they often require. R API packages are being developed quickly and should be your first search when looking to request data via an API. As illustrated, these packages tend to be very easy to work with. However, when you want to leverage an organization's API that has not been integrated into an R package, the `httr` package provides a convenient way to request data.

<small><a href="#">Go to top</a></small>
