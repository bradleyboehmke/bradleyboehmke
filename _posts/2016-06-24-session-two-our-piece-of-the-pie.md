---
layout: post
title:  Session 2&#58; Our Piece of the Pie
date: 2016-06-27 10:49:18
published: true
categories: [harvard]
tags: [continuing-eduction]
---

## Preparation for Session #2

- DATE: Monday, June 27
- TIME: 9:00 – 10:30 pm
- CLASS: Theory of Change: Our Piece of the Pie
- FACULTY: Julie Wilson
- CASE: [Our Piece of the Pie: From Data to Decison-Making](https://www.dropbox.com/s/u1b0lw2lkzo1w93/Case%20-%20Wilson_Our%20Piece%20of%20the%20Pie_From%20Data%20to%20Decision%20Making.pdf?dl=0)

## Study Questions

**Q1:** How effective a program do you think Our Piece of the Pie is now?  What are the criteria by which you made your decision about its effectiveness?  Along what dimensions do you thinkeffectiveness should be measured?

**A1:** Unknown at this point. I define *effectiveness* as having an impact on some output. So in OPP I would consider improved effectiveness as increasing youth attendance in school, increased graduation rates, increased post-secondary education, increased youth employment, etc. These our the outputs that OPP was designed to impact so for the data driven decision-making environment to improve effectiveness we would need to see if it impacts these attributes. 

**Q2:** How efficient is Our Piece of the Pie?  How would you explain the difference between efficiency and effectiveness?

**A2:** First, I define *effectiveness* as the ability to produce the desired outputs that a program is designed to produce. So for OPP I would categorize their main outputs as being increasing youth attendance in school, increased graduation rates, increased post-secondary education, increased youth employment, etc.  *Efficiency*, on the other hand, is the ability to use resource inputs to produce program outputs. The more optimal your processes use inputs to produce outputs the more efficient you are operating.

So is OPP operating efficiently? Partly yes and partly no.  First, I think the data initiative caused certain processes to become more efficient such as collecting and summarizing program cost and performance, budget tracking and reporting, and identify potential areas for further investigation. I consider these more efficient because OPP personnel can now do these activities in less time and effort regardless of whether they have an impact on effectiveness.  However, I also consider these processes not necessarily the primary outputs (i.e. youth program performance metrics). This is why I say partly no; because it remains uncertain if the primary measures of *effectiveness* (or outputs) actually increase with the same level (or less) of resource inputs. 

**Q3:** What should the relationship be between an organization’s theory of change and performance measurement and between performance measurement and program evaluation or other forms of assessment?

**A3:** An organization's theory of change should drive the specific activities, capabilities, and outcomes that require measurement. Any performance measurement metric data should be targeted towards measuring these three components. Having this data will allow decision makers that ability to track changes in these activities when new initiatives are implemented.

> *"A robust theory of change speci es the set of programs, activities, organizational capabilities, and relationships required to achieve the outcomes the organization will hold itself accountable for. Once this theory is explicit and agreed upon, it is much easier to collect the full set of information that reveals not only whether the organization is achieving its outcomes, but also why it achieves them, and what needs to change to improve."* - Measurement as Learning (p. 2)

Furthermore, these performances metrics should also flow through to any program evaluations. In essence, these metrics should be built in a way to explain/illustrate the performance of programs.

**Q4:** What are the challenges to collecting, analyzing and presenting data in ways that inform decisions from the front line to the CEO?  To what extent and in what ways does collecting data and analyzing performance enhance and detract from pursuing a service provider’s mission?

**A4:** Many organizations have disparate data collection efforts.  This makes it challenging to combine different data that by themselves are not informative for decisions but combined are. This requires sometimes difficult data munging efforts that can be time intensive. Also, data are not always collected at the level where the decision needs to be made which causes data aggregation to take place. This can lead to information loss or sometimes challenges of aggregation on data that is not easy to aggregate.

Data presentation is also a critical component. Dashboards and other reporting mechanisms provide understanding of the state of the metric(s) being reported but do not always tell the whole story for the executives. In the OPP example they integrated key questions to consider when looking at the metrics, which helps provide perspective and some context for the decision makers. However, many times reporting mechanisms are meant to drive more questions which requires further analysis.  So simply reporting performance is not enough, a continual dialogue, investigation, and analysis is required to truly enhance service.

Lastly, data collection and performance measurement efforts must serve multiple levels and stakeholders of the organization. If performance measurement efforts are only integrated to serve the c-suite level then it will likely be rejected by lower levels. Effort needs to be made to make the data collection and performance measurement system serve the tactical, operational, and strategic level stakeholders.

**Q5:** What are the challenges to changing the culture of an organization to become one more focused on performance measurement and evidence generation?

**A5:** First, it's critical to have complete buy-in from leadership and stakeholders regarding the idea of performance measurement and evidence-based insights. Without buy-in it will be difficult to get the time, resources, and environments required to create a data culture.  

> *"An organization's leaders must, through their actions, embrace measurement themselves and purposefully provide staff with the time, resources, incentives, and 'learning forums' to do the same."* - Measurement as Learning (p. 3)

Changing the culture takes time. This should not be thought of as a quick-turn effort and, truthfully, is a continual process. First, getting the effort off the ground will take considerable time and effort both from leadership to be fully engaged with what should be measured and also from the operational level regarding the manual effort of creating/curating a data collection and measurement system to collect the desired information. Second, this is a continual process of collecting and using data and then refining it by eliminating unnecessary data.

Changing the culture takes resources. Manpower efforts will be required to develop the performance measurement system. Leadership should be willing to dedicate personnel to these efforts rather than just making it a side-job. 

Changing culture requires money. This may include cost of software or hiring new personnel to develop and manage data collection and performance measurement processes.

>  *"Culture matters far more than systems. If your organization doesn't care about metrics, don't bother to start building systems to measure performance."* - Brian Trelstad, CIO Acumen Fund


## Session Notes

**Question zero**: ten words to concisely state the overarching goal of what you are trying to do


**Theory of change** is your roadmap to accomplishing your question zero. Its the cause-n-effect linkages.

Theory of change in this case:

- IF we can get them (the youth) to come
   - THEN we can assess their needs/status
   - THEN we can divide them into segments
   - THEN we can design curriculum/support
   - THEN we can deliver the curriculum/support
   - THEN we can keep the youth involved in education
   - THEN we can turn them into law-abiding, self-sustaining adults
   
**Logic model**: inputs &#8594; process &#8594; outputs &#8594; outcomes

- Inputs
   - youth
   - trained patient staff
   - funding
   - space
   - technology
   - etc

- Process
   - segmenting youth
   - recruiting youth
   - developing programming
   - etc
   
- Outputs
   - Attendance
   - budget execution
   
- Outcomes
   - Academics (GEDs, diplomas)
   - Employment
   - Participation growth
   - Criminal activities
   - Staff turnover/qualifications

Performance measurement metrics needs to be linked to your logic model outputs and outcomes

Ultimately we need to measure the **impact** of the program

- impact simply means that the program made a difference
- RCTs - randomization is preferred so we can compare to the counter factual





